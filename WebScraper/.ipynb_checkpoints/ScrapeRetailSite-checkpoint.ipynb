{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search product name: embossed sheet sets\n",
      "\n",
      " product name: EMBOSSED SHEET SETS                          \n",
      " price: $35.00                          \n",
      " details: No wrinkles in time. Enjoy the neat, crisp look of this sheet from Sanders featuring a unique embossed print.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build a web scraper for one of the retail websites: macys.com.\n",
    "\n",
    "Author: Ye N.E.\n",
    "\n",
    "Description: \n",
    "    (1) The web scraper should scrape Macys.com \n",
    "        whenever you run your program, scrape all of the products \n",
    "        listed on Macy's homepage, \n",
    "        and save the following data for each product \n",
    "        in a CSV file: the product name, price, and description.\n",
    "    \n",
    "    (2) The information should come from the product pages \n",
    "        (such as here: https://mcys.co/2GiLTRq)\n",
    "        \n",
    "    (3) Your program needs a function that allows me to search \n",
    "        by name all of the products in the CSV file. \n",
    "        If a product is found, your program should print \n",
    "        the product name, price, and description. \n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import requests\n",
    "import csv\n",
    "import apikey\n",
    "import os\n",
    "\n",
    "        \n",
    "class Scraper: \n",
    "    def __init__(self, site=\"https://www.macys.com\"):\n",
    "        self.site = site \n",
    "        self.Categories = set()\n",
    "        self.URLs = set()\n",
    "        self.products = []\n",
    "        self.soup = None\n",
    "        \n",
    "    def get_parse(self, site): \n",
    "        agent = {\"User-Agent\":\"Mozilla/5.0\"} \n",
    "        my_api_key = apikey.Key()\n",
    "        payload = {'api_key':my_api_key, 'url':site}\n",
    "        page = requests.get(site, headers=agent)\n",
    "        \n",
    "        #For when using Scraper API to avoid getting blocked \n",
    "#         page = requests.get('http://api.scraperapi.com', \n",
    "#                             headers = agent,\n",
    "#                             params = payload, timeout=60) \n",
    "\n",
    "        self.soup = Soup(page.content, \"html.parser\")\n",
    "    \n",
    "\n",
    "    def get_url_categories(self):\n",
    "        self.get_parse(self.site)\n",
    "        soup = self.soup\n",
    "        #print(soup)\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            path = tag[\"href\"]\n",
    "            if \"http\" not in path\\\n",
    "                and \"COL\" in path\\\n",
    "                and \"/shop/\" in path:\n",
    "                self.Categories.add(self.site+path)\n",
    "        \n",
    "        return self.Categories\n",
    "    \n",
    "    \n",
    "    def get_url_products(self):\n",
    "        self.get_url_categories()\n",
    "#         test = self.Categories.pop()\n",
    "#         self.Categories.add(test)\n",
    "#         self.get_parse(test)\n",
    "#         soup = self.soup\n",
    "        for url in self.Categories:\n",
    "            self.get_parse(url)\n",
    "            soup = self.soup\n",
    "            for tag in soup.find_all(\"a\", {\"class\": \"productDescLink\"}):\n",
    "                path = tag.get(\"href\")\n",
    "                self.URLs.add(self.site+path)\n",
    "        \n",
    "        return self.URLs\n",
    "            \n",
    "        \n",
    "    def scrape(self):\n",
    "        self.get_url_products()\n",
    "        for url in self.URLs:\n",
    "            self.get_parse(url)\n",
    "            soup = self.soup\n",
    "            try: \n",
    "                name = (((soup.find_all(\"h1\", {\"class\": \"p-name h3\"})[0].text)\\\n",
    "                        .replace(\"\\n\",\"\")).strip()).upper()\n",
    "                price = ((soup.find_all(\"div\", {\"class\": \"price\"})[0].text)\\\n",
    "                         .replace(\"\\n\",\"\")).strip()\n",
    "                des = ((soup.find_all(\"p\", {\"data-auto\": \"product-description\"})[0].text)\\\n",
    "                       .replace(\"\\n\",\"\")).strip()\n",
    "                self.products.append([name, price, des])\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "        return self.products\n",
    "    \n",
    "    \n",
    "    def save_cvs(self):\n",
    "        self.scrape()\n",
    "        with open(\"macys-products.cvs\", \"w+\") as csvf:\n",
    "            w = csv.writer(csvf, delimiter=\",\")\n",
    "            for i in self.products:\n",
    "                w.writerow(i)\n",
    "                \n",
    "    \n",
    "    def get_product_info(self, name):\n",
    "        name = str(name).upper()\n",
    "        with open(\"macys-products.cvs\", \"r\") as csvf:\n",
    "            r = csv.reader(csvf, delimiter=\",\")\n",
    "            for row in r:\n",
    "                if row[0]==name:\n",
    "                    print(\"\\n product name: {}\\\n",
    "                          \\n price: {}\\\n",
    "                          \\n details: {}\"\\\n",
    "                          .format(row[0], row[1], row[2]))    \n",
    "            \n",
    "            \n",
    "scrape = Scraper()\n",
    "\n",
    "#=================Scrape and save data from macys.com=============\n",
    "#scrape.scrape()\n",
    "scrape.save_cvs()\n",
    "\n",
    "#==============Search product info by name========================\n",
    "product_name = input(\"Search product name: \")\n",
    "scrape.get_product_info(product_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
